{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d942a26-9611-45ca-9ccf-dc53462660c8",
   "metadata": {},
   "source": [
    "# Neural Network - Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4fceb-a07a-4cf5-ab07-32cfa1cc6516",
   "metadata": {},
   "source": [
    "## 0 - Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6798a0c2-708f-4d1f-a7ac-79ab46cbdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afbfac8-1984-487b-a8eb-544e0a356731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe597761-c664-4870-9682-442247d7f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testCases import *\n",
    "from public_tests import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b25f13-a7ed-4d7d-b08b-a02c1d8ba383",
   "metadata": {},
   "source": [
    "## 1 - Math Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd2daf-fde4-46da-841e-a68bfe0c8366",
   "metadata": {},
   "source": [
    "The most critical part in the neural network is the backward propagation.\n",
    "\n",
    "We can use the numerical computation of gradient:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{\\mathcal{L}(\\theta + \\varepsilon) - \\mathcal{L}(\\theta - \\varepsilon)}{2 \\varepsilon} \n",
    "$$\n",
    "\n",
    "to verify if our backward propagation implementation works as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb00dce-d304-4eb5-b685-fba90212251c",
   "metadata": {},
   "source": [
    "## 2 - Gradient Check Implementation: 1D Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6b82e-b316-4b78-b26c-082fb1a545f6",
   "metadata": {},
   "source": [
    "`forward_propagation` to compute $\\mathcal{L} = X \\cdot \\theta$ where $X, \\theta \\in R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150098f4-20db-4205-9401-4b343e977379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x: float, theta: float):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: float to denote the input data\n",
    "    theta: float to denote the weight parameter\n",
    "    \n",
    "    Returns:\n",
    "    L: the result of the function\n",
    "    \"\"\"\n",
    "    L = theta * x\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31708e-64ec-466a-b36f-cf3773c57f19",
   "metadata": {},
   "source": [
    "`backward_propagation` to compuate the derivative of $\\mathcal{L}$ with respect to $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7518f3f8-fa88-45dd-ae82-eb3c98ca189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x: float, theta: float):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: float to denote the input data\n",
    "    theta: float to denote the weight parameter\n",
    "    \n",
    "    Returns:\n",
    "    dtheta: the derivatice\n",
    "    \"\"\"\n",
    "    dtheta = x\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c05b89-64f9-46ac-8fbe-21d2e471ce1d",
   "metadata": {},
   "source": [
    "`gradient_check` performs numerical computation on the gradient and checks the difference between numerical and actual ones\n",
    "\n",
    "- $d\\theta$: actual derivative\n",
    "- $d\\hat{\\theta}$: numerical computation result\n",
    "- $\\text{diff} = \\frac{||d\\theta - d\\hat{\\theta}||_2}{||d\\theta||_2 + ||d\\hat{\\theta}||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3329d9-68f9-4f5a-a561-9085355008f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x: float, theta: float, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: float to denote the input data\n",
    "    theta: float to denote the weight parameter\n",
    "    epsilon: small float for numerical computation of the gradient\n",
    "    \n",
    "    Returns:\n",
    "    difference: the difference between numerical results and actual one\n",
    "    \"\"\"\n",
    "    # Numerical computation\n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    L_plus = theta_plus * x\n",
    "    L_minus = theta_minus * x\n",
    "    dtheta_hat = (L_plus - L_minus) / (2 * epsilon)\n",
    "    # True gradient\n",
    "    dtheta = backward_propagation(x, theta)\n",
    "    # Calculate the difference\n",
    "    numerator = np.linalg.norm(dtheta_hat - dtheta)\n",
    "    denominator = np.linalg.norm(dtheta_hat) + np.linalg.norm(dtheta)\n",
    "    difference = numerator / denominator\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b8352-9f2a-493a-bdbe-fe31fc7b212a",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5724e5da-89b5-417d-933f-50a06a22920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.919335883291695e-10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(2,4)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b941ec6-b5ae-4262-b893-bbacd7699206",
   "metadata": {},
   "source": [
    "## 3 - Gradient Check Implementation: NDimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fbf0d-cb4d-4a83-9ea2-c40f447b4703",
   "metadata": {},
   "source": [
    "Imagine we have three-layer fully-connected neural network:`linear` + `relu` -> `linear` + `relu` -> `linear` + `sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097a391-8a72-477f-a3b2-514959930fc6",
   "metadata": {},
   "source": [
    "`forward_propagation_n` to compute output from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c180b8a-8405-4185-88cd-3110a803cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X: np.ndarray, Y: np.ndarray, parameters: dict):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: (num_features, num_samples)\n",
    "    Y: (1, num_samples)\n",
    "    parameters: {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    \n",
    "    Returns:\n",
    "    loss: the float to denote loss function result\n",
    "    cache: (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \"\"\"\n",
    "    # Get parameters\n",
    "    num_samples = X.shape[1]\n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "    W3, b3 = parameters[\"W3\"], parameters[\"b3\"]\n",
    "    # 3-layer: linear + relu -> linear + relu -> linear + sigmoid\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    # Loss\n",
    "    loss = -1.0 / num_samples * (np.matmul(Y, np.log(A3).T) + np.matmul((1 - Y), np.log(1 - A3).T))\n",
    "    # save for return\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e81e2-da8f-4143-aabd-874c83b1389c",
   "metadata": {},
   "source": [
    "`forward_propagation_n` to perform the backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdc35b1-b49b-41da-bd04-7155d71b43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X: np.ndarray, Y: np.ndarray, cache: tuple):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: (2, num_samples)\n",
    "    Y: (1, num_samples)\n",
    "    cache: (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    Returns:\n",
    "    gradients: {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \"\"\"\n",
    "    # Get info\n",
    "    num_samples = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    # Backward propagation - the last layer\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1 / num_samples * np.matmul(dZ3, A2.T)\n",
    "    db3 = 1 / num_samples * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    # Backward propagatino - the 2nd layer\n",
    "    dA2 = np.matmul(W3.T, dZ3)\n",
    "    dZ2 = dA2 * np.where(Z2 > 0, 1, 0)\n",
    "    dW2 = 1 / num_samples * np.matmul(dZ2, A1.T)\n",
    "    db2 = 1 / num_samples * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # Backward propagatino - the 1st layer\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * np.where(Z1 > 0, 1, 0)\n",
    "    dW1 = 1 / num_samples * np.dot(dZ1, X.T)\n",
    "    db1 = 1 / num_samples * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    # Save for return\n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eba1a2-65d8-499f-ad3f-b8976571942d",
   "metadata": {},
   "source": [
    "`gradient_check` performs numerical computation on the gradient and checks the difference between numerical and actual ones\n",
    "\n",
    "- $d\\theta$: actual derivative\n",
    "- $d\\hat{\\theta}$: numerical computation result\n",
    "- $\\text{diff} = \\frac{||d\\theta - d\\hat{\\theta}||_2}{||d\\theta||_2 + ||d\\hat{\\theta}||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9730a8a2-c8fe-430d-bc55-52df2d51a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters: dict, gradients: dict, X: np.ndarray, Y: np.ndarray, epsilon: float=1e-7):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters: {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    X: (num_features, num_samples)\n",
    "    Y: (1, num_samples)\n",
    "    gradients: {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    epsilon: small float number used to calculate the derivative\n",
    "    \n",
    "    Returns:\n",
    "    difference: the difference between numerical results and actual one\n",
    "    \"\"\"\n",
    "    # Flatten the weight and bias parameter into a vector\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    # Flatten the gradients into a vector    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    # Get info\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    # Verify parameter and gradient vectors have the same dimension\n",
    "    assert parameters_values.shape[0] == grad.shape[0], f'parameters and grad shall have identical dimension.'\n",
    "    # Initialize to store results\n",
    "    L_plus = np.zeros((num_parameters, 1))\n",
    "    L_minus = np.zeros((num_parameters, 1))\n",
    "    grad_approximate = np.zeros((num_parameters, 1))\n",
    "    # Compute approximate gradients: we need to rely on explicit for loop to change one parameter at a time to get the derivative\n",
    "    for i in range(num_parameters):\n",
    "        # Calculate the loss function corresponding to theta_plus\n",
    "        theta_plus = parameters_values.copy()\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        L_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "        # Calculate the loss function corresponding to theta_minus\n",
    "        theta_minus = parameters_values.copy()\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        L_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        # Numerical result of gradient\n",
    "        grad_approximate[i] = (L_plus[i] - L_minus[i]) / (2*epsilon)\n",
    "    # Calculate the difference\n",
    "    numerator = np.linalg.norm(grad_approximate - grad)\n",
    "    denominator = np.linalg.norm(grad_approximate) + np.linalg.norm(grad)\n",
    "    difference = numerator / denominator\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b268e583-5e65-463a-ae42-32a521b3e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y, 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9bc163-d63e-48d5-8271-c3a3df723d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1885552035482147e-07"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f632843-63de-40df-9826-0ffc295f7f1d",
   "metadata": {},
   "source": [
    "## 4 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16466486-08fa-414d-ad4c-ea94ccba0839",
   "metadata": {},
   "source": [
    "When defining backward propagation:\n",
    "\n",
    "- Use numerical computation to verify the gradients are correct.\n",
    "- The gradient checking shall rely on an explicit for loop such that we ONLY change one parameter at one time to get its corresponding derivative.\n",
    "- Once gradient checking is done, we do not use it in the actual training loop because it is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c15b3-b899-4aee-934d-0afd2c6fe93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
