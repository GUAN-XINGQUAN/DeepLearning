{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33eae4d-c49a-4317-9dec-24c4deccf865",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network Breakdown by Compnoent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fe409-7475-41f2-a897-d7d18ca23d3d",
   "metadata": {},
   "source": [
    "We will focus on main components in the neural network instead of performing an actual regression or classification work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1c22a-653c-4b88-9aa8-70d24ee65d1a",
   "metadata": {},
   "source": [
    "## 0 - Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e13f91-ebc0-4bda-8096-8ee2eee35e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61076d31-bcea-49aa-9019-20c8a5c76ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ccff3-8309-4b1c-bcbf-b6e3a0c34302",
   "metadata": {},
   "source": [
    "The following files are auxiliary files for unit test. For copyright issues, they won't be provided in this repo. However, they only serve as testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5afabe-7751-40cd-bbc2-bdb31d93b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from public_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a716297-994f-4ca5-bdd9-b491d3825d21",
   "metadata": {},
   "source": [
    "## 1 - Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864f9f8-d24f-41aa-92b3-366a35e094ad",
   "metadata": {},
   "source": [
    "`sigmod` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafa308f-c4eb-40fa-ad8f-1241a7959f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a770c-a3b5-482b-8106-2a33790d77c0",
   "metadata": {},
   "source": [
    "`relu` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b68fe17-765b-46d8-990a-8f80ade1d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Be cautious: we need to use np.maximum NOT np.max\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f16f05-557d-48d5-bb24-bd5e3c298dc6",
   "metadata": {},
   "source": [
    "## 2 - Initialize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27591b01-26f3-4caf-9fa6-12bb1ce8f530",
   "metadata": {},
   "source": [
    "`init_param` to initialize parameters for neural network with any number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1e8ff5-ee9c-481a-9f6f-55b93a583822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(layer_size: list[int]):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    layer_size: a list of integers representing number of neurons from input layer to output layers.\n",
    "                length of this list equals to the number of layers (including input and output layer)\n",
    "    \n",
    "    Returns:\n",
    "    parameters: {'W1': W1, 'b1': b1', ..., 'Wm': Wm, 'bm': bm}\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    num_layer = len(layer_size)\n",
    "    parameters = {}\n",
    "    for layer in range(1, num_layer):\n",
    "        # initialize the parameters\n",
    "        W = np.random.randn(layer_size[layer], layer_size[layer-1]) * 0.01\n",
    "        b = np.zeros((layer_size[layer], 1))\n",
    "        # save for return\n",
    "        parameters.update({\n",
    "            f'W{layer}': W,\n",
    "            f'b{layer}': b\n",
    "        })\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988b5b9-6649-4580-a89f-c500159ab799",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be22c34-abe3-441f-ab78-0181d9ccc579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "parameters = init_param([5,4,3])\n",
    "initialize_parameters_deep_test(init_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caa58b-1673-43fe-800e-e978ca3b90d5",
   "metadata": {},
   "source": [
    "## 3 - Forward Propagataion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb7c13-fc76-4eb0-af01-8acf393f9438",
   "metadata": {},
   "source": [
    "Forwward propagation is to compute the transformation for input. The formula are:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[l]} = \\text{relu}(Z^{[l]})\n",
    "$$\n",
    "or \n",
    "$$\n",
    "A^{[l]} = \\text{sigmoid}(Z^{[l]}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee16ce-910c-41c4-ab46-fa86435dbbdb",
   "metadata": {},
   "source": [
    "`linear_forward` represent a single module of linear transformation without involving activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684df683-83a9-4fdf-a241-a42f90f69949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A: np.ndarray, W: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    W: weight matrix from previous layer to current layer: (current_layer_size, previous_layer_size)\n",
    "    A: the output from previous layer (should be original data if previous layer is input layer): (previous_layer_size, num_samples)\n",
    "    b: bias vector for the current: (current_layer_size, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z: the results after linear transformation\n",
    "    cache: {'W': W, 'A': A, 'b': b}\n",
    "    \"\"\"\n",
    "    Z = np.matmul(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4206590-af3f-47b3-8447-37bb73ef921d",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b14317c-5fb1-4a74-a916-3b9e189eedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_A, t_W, t_b = linear_forward_test_case()\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "linear_forward_test(linear_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af5ebb-b315-4f90-a3a1-d3c55e91ca8f",
   "metadata": {},
   "source": [
    "`linear_activate_forward` represents a single module of linear transformation PLUS activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd14ca9-9c2a-469b-b710-c00bb4633fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activate_forward(A: np.ndarray, W: np.ndarray, b: np.ndarray, activation_func: str):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    W: weight matrix from previous layer to current layer: (current_layer_size, previous_layer_size)\n",
    "    A: the output from previous layer (should be original data if previous layer is input layer): (previous_layer_size, num_samples)\n",
    "    b: bias vector for the current: (current_layer_size, 1)\n",
    "    activation_func: 'sigmoid' or 'relu'\n",
    "    \n",
    "    Returns:\n",
    "    A_cur: the results after linear transformation and activation\n",
    "    cache: {'W': W, 'A': A, 'b': b, 'Z': Z}\n",
    "    \"\"\"\n",
    "    Z, cache = linear_forward(A, W, b)\n",
    "    if activation_func == 'sigmoid':\n",
    "        A_cur, activation_cache = sigmoid(Z)\n",
    "    elif activation_func == 'relu':\n",
    "        A_cur, activation_cache = relu(Z)\n",
    "    else:\n",
    "        raise Exception(f'The input {activation_func} is not supported.')\n",
    "    cache = (cache, activation_cache)\n",
    "    return A_cur, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755215c-56b4-4f64-b013-7ca1c8db6f71",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c3761f-bd40-4373-aecb-8efe011e16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_A_prev, t_W, t_b = linear_activation_forward_test_case()\n",
    "t_A, t_linear_activation_cache = linear_activate_forward(t_A_prev, t_W, t_b, 'sigmoid')\n",
    "t_A, t_linear_activation_cache = linear_activate_forward(t_A_prev, t_W, t_b, 'relu')\n",
    "linear_activation_forward_test(linear_activate_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65866175-32e0-425c-824b-51df0ae2ed58",
   "metadata": {},
   "source": [
    "`forward` function to combine multiple linear forward plus activation modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcdc48f9-62fa-471b-b9e1-bd0dd2ba99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, parameters: dict):\n",
    "    \"\"\"\n",
    "    linear transformation -> relu -> linear .... -> relu -> linear -> sigmoid\n",
    "\n",
    "    Arguments:\n",
    "    parameters: {'W1': W1, 'b1': b1', ..., 'Wm': Wm, 'bm': bm}\n",
    "    X: (num_features, num_samples)\n",
    "    \"\"\"\n",
    "    # Get the total number of layers\n",
    "    num_layer = len(parameters) // 2\n",
    "    # Save cache\n",
    "    A, caches = X, []\n",
    "    # From input layer to before the output layer\n",
    "    for layer in range(1, num_layer):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activate_forward(A_prev, parameters[f'W{layer}'], parameters[f'b{layer}'], 'relu') \n",
    "        caches.append(cache)\n",
    "    # Implement the output layer\n",
    "    AL, cache = linear_activate_forward(A, parameters[f'W{num_layer}'], parameters[f'b{num_layer}'], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3db8a-9851-4adf-820a-c3c849c5d57d",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "648a99d7-095b-4437-9cfe-489d28e64dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_X, t_parameters = L_model_forward_test_case_2hidden()\n",
    "t_AL, t_caches = forward(t_X, t_parameters)\n",
    "L_model_forward_test(forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb47562-6132-44a1-afd7-b1165b04c280",
   "metadata": {},
   "source": [
    "## 4 - Loss Function (Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33009bec-78ba-4255-947b-fab6232a0e2c",
   "metadata": {},
   "source": [
    "$$\n",
    "J = -\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299f8c93-e8d7-46aa-b20a-f365b7511dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(AL: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL: output of the last layer (1, number of examples)\n",
    "    Y: ground truth (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    loss: the loss value calculated over all samples\n",
    "    \"\"\"\n",
    "    num_samples = AL.shape[1]\n",
    "    loss = 1 / num_samples * np.sum(- np.dot(Y, np.log(AL.T)) - np.dot((1 - Y), np.log(1 - AL.T)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99040f8b-f24b-475f-98fd-25cc2ea1cf70",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ab5b5a-9308-43cc-bb16-bf5f97a783a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_Y, t_AL = compute_cost_test_case()\n",
    "t_cost = compute_loss(t_AL, t_Y)\n",
    "compute_cost_test(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8500b85-eae1-4eb7-b03c-3461f279172a",
   "metadata": {},
   "source": [
    "## 5 - Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e05cd-1537-4ad1-98ab-61cdb882c488",
   "metadata": {},
   "source": [
    "The backward propagation is to compute the gradients with respect to each parameter. The formula are listed below:\n",
    "\n",
    "$$ \n",
    "dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "496bd22a-1896-4a6f-a568-1f0236b39ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Perform backward propagation once dZ is given.\n",
    "\n",
    "    Arguments:\n",
    "    dZ: gradient of the cost with respect to Z in the current layer: same shape with Z\n",
    "    cache: (A_prev, W, b) from forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev: gradient with respect to the activation, same shape with A_prev\n",
    "    dW: gradient with respect to W, same shape as W\n",
    "    db: gradient with respect to b, same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    num_samples = A_prev.shape[1]\n",
    "    dW = 1 / num_samples * np.matmul(dZ, A_prev.T)\n",
    "    db = 1 / num_samples * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.matmul(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee94f3-fdfa-4233-8085-eea9cc04046a",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd47ef1-f41e-4291-acea-89693f34a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_dZ, t_linear_cache = linear_backward_test_case()\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "linear_backward_test(linear_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc930e76-ef51-456a-9ded-e761d668a297",
   "metadata": {},
   "source": [
    "`linear_activate_backward` computes the backward propagation with activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ca5658-1d28-4793-b666-51c76c4d72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activate_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA: gradient for the current layer\n",
    "    cache: tuple of values (linear_cache, activation_cache)\n",
    "    activation : \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradient with respect to A in the previous laywer\n",
    "    dW: gradient with respect to W in the current layer\n",
    "    db: gradient with respect to b in the current layer\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    else:\n",
    "        raise Exception(f'The input {activation} is not supported.')\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6d9ef-aa66-43ca-aeeb-6c3733ab9167",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e41cd45-5981-4702-ace2-e5c8500e5725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n",
    "t_dA_prev, t_dW, t_db = linear_activate_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "t_dA_prev, t_dW, t_db = linear_activate_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "linear_activation_backward_test(linear_activate_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1ae1a-0fd4-4fcc-852e-5c3bba305f6e",
   "metadata": {},
   "source": [
    "`backward` combines multiple backward modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4b5af4-fa1a-4ebb-9440-c01f4b6f5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for multiple times.\n",
    "\n",
    "    Arguments:\n",
    "    AL: the probability output of the last layer sigmoid: (1, num_samples)\n",
    "    Y: groud truth: (1, num_samples)\n",
    "    caches: a list of caches including linear_activate_forward cache results.\n",
    "\n",
    "    Returns:\n",
    "    grads: {'dA1': dA1, 'dW1': dW1, 'db1': db1', ..., 'dAL': dAL, 'dWL': dWL, 'dbL': dbL}\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    num_layer = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    # The gradient of cross entropy loss function\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    # The backward propagation in the last layer\n",
    "    cur_cache = caches[-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activate_backward(dAL, cur_cache, 'sigmoid')\n",
    "    grads[f'dA{num_layer-1}'] = dA_prev_temp\n",
    "    grads[f'dW{num_layer}'] = dW_temp\n",
    "    grads[f'db{num_layer}'] = db_temp\n",
    "    # From the second from the last layer\n",
    "    for layer in range(num_layer-2, -1, -1):\n",
    "        cur_cache = caches[layer]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activate_backward(dA_prev_temp, cur_cache, 'relu')\n",
    "        grads[f'dA{layer}'] = dA_prev_temp\n",
    "        grads[f'dW{layer+1}'] = dW_temp\n",
    "        grads[f'db{layer+1}'] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca907df-8141-4c94-b80a-b15414803ccc",
   "metadata": {},
   "source": [
    "`TEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac6774e2-6e84-4654-ba7c-0496f56ffdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_AL, t_Y_assess, t_caches = L_model_backward_test_case()\n",
    "grads = backward(t_AL, t_Y_assess, t_caches)\n",
    "L_model_backward_test(backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67396ed-e574-4dd1-a56f-c0437821f128",
   "metadata": {},
   "source": [
    "## 6 - Update Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfa2ac-c2be-4595-89ab-54a9858a4773",
   "metadata": {},
   "source": [
    "`update_param` function to update the parameters using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "595935b2-604b-4680-a46c-e8e327c6119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param(params: dict, grads: dict, learning_rate: float=1e-3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: {'W1': W1, 'b1': b1', ..., 'Wm': Wm, 'bm': bm}\n",
    "    grads: {'dA1': dA1, 'dW1': dW1, 'db1': db1', ..., 'dAL': dAL, 'dWL': dWL, 'dbL': dbL}\n",
    "    \n",
    "    Returns:\n",
    "    parameters: {'W1': W1, 'b1': b1', ..., 'Wm': Wm, 'bm': bm}\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    num_layer = len(params.keys()) // 2\n",
    "    # Gradient descent\n",
    "    for layer in range(num_layer):\n",
    "        parameters[f'W{layer+1}'] = parameters[f'W{layer+1}'] - learning_rate * grads[f'dW{layer+1}']\n",
    "        parameters[f'b{layer+1}'] = parameters[f'b{layer+1}'] - learning_rate * grads[f'db{layer+1}']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a417cbf6-dfcd-4e0a-9779-8579c7b5a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_parameters, grads = update_parameters_test_case()\n",
    "t_parameters = update_param(t_parameters, grads, 0.1)\n",
    "update_parameters_test(update_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
